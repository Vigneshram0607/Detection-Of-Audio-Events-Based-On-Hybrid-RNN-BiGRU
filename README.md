# Detection-Of-Audio-Events-Based-On-Hybrid-RNN-BiGRU
Learning audio sequence representations for acoustic event classification

The classification of the acoustic events happening around us, is important for monitoring and recognising them individually to know every minute things. Previous works on audio event classification utilized some supervised pre-training and meta-learning systems that depend significantly on labelled data Here we contribute our work for acoustic event classification (AEC) using an unsupervised hybrid RNN-BiGRU learning framework which is used in ascertaining the audio sequences in vector format for the Audio Event Classification. We train audio representations from a vast quantity of unlabeled data and apply the results for few-shot AEC. As a result, we frequently use the Long Short Term Memory (LSTM) architecture, which is an RNN architecture used in deep learning methods. LSTM has the feedback connections that has the capability to process a single data point, aside from that it can also progress the whole data sequences which is why LSTM is utilized in the AEC. Furthermore GRUs are very similar to LSTM which uses gates to control the flow of information. This approach reduces the classification errors for any acoustic condition. Furthermore, detecting and classifying noises other than speech might help improve the robustness of speech technologies like automatic speech recognition.

## Table Of Contents:
1. Introduction to Audio Classification

2. Project Overview

3. Dataset Overview

4. Hands-on Implementing Audio Classification project

    -> EDA On Audio Data
    
    -> Data Preprocessing
    
    -> Building RNN-BiGRU for Audio Classification
    
5.Testing some unknown Audio

6.End Notes



